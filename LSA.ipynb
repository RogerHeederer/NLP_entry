{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLV4DBROe+Axy+VmgFcObl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RogerHeederer/NLP_entry/blob/master/LSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6MuONgL_TLY",
        "colab_type": "text"
      },
      "source": [
        "Reference Source : wikidocs.net 유영준님 자료\n",
        "\n",
        "스스로 학습하면서 필요한 부분에는 추가적 설명, 소스 코드 삽입 및 수정 등이 있습니다. 영리적 목적이 아닌, 자기 계발 목적으로 정리한 자료입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY3t9aaP_XwK",
        "colab_type": "text"
      },
      "source": [
        "##Topic Modeling : 통계적 모델의 하나로, 텍스트 본문의 숨겨진 의미 구조를 파악하기 위해 사용되는 텍스트 마이닝 기법\n",
        "\n",
        "**LSA는 토픽 모델링 분야에 아이디어를 제공한 알고리즘**\n",
        "\n",
        "Latent Semantic Indexing or Latent Semantic Analysis\n",
        "\n",
        "* 전치행렬, 단위행렬, 역행렬, 직교행렬, 대각행렬 개념이 나온다\n",
        "\n",
        "* 절단된 SVD (Truncated SVD)를 사용\n",
        "\n",
        "* 기존의 DTM이나 TF-IDF는 단어의 의미를 전혀 고려하지 못하는 단점. 그래서 LSA는 위 DTM, TF-IDF에 절단된 SVD를 적용해서 차원을 축소시키고, 단어들의 잠재적 의미를 이끌어낸다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYECD_ZV_Nt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "adc958ef-5847-4dee-ded7-c639410e81b4"
      },
      "source": [
        "import numpy as np\n",
        "A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
        "print(A)\n",
        "np.shape(A) # 4*9 DTM Document Term Matrix\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 0 1 1 0 0]\n",
            " [0 0 0 1 1 0 1 0 0]\n",
            " [0 1 1 0 2 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 1 1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rb5rnxHGBWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "U, s, VT = np.linalg.svd(A, full_matrices = True) #s 는 대각행렬 변수  VT 전치행렬 U 직교행렬"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfbmJaBvHIYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "121e41b8-49f9-4c73-98ae-9ec83739c16e"
      },
      "source": [
        "print(U.round(2), '\\n\\n', s.round(2), '\\n\\n', VT.round(2)) # 소수점 2째자리까지만"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.24  0.75  0.   -0.62]\n",
            " [-0.51  0.44 -0.    0.74]\n",
            " [-0.83 -0.49 -0.   -0.27]\n",
            " [-0.   -0.    1.    0.  ]] \n",
            "\n",
            " [2.69 2.05 1.73 0.77] \n",
            "\n",
            " [[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
            " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
            " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
            " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
            " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
            " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
            " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
            " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
            " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0F8qpj-HJ6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "cc3d920e-bd93-4a61-93a1-1f52671b839b"
      },
      "source": [
        "S = np.zeros((4,9)) \n",
        "S[:4, :4] = np.diag(s)\n",
        "print(S.round(2))\n",
        "np.shape(S)\n",
        "\n",
        "# 특이값 리스트 s를 행렬 형태료 편경  2.69 > 2.05 >1.73 > 0.77 내림차순 확인"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w4C5_YuJWYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "6faee0a3-b382-4d69-d1b8-99dfc8f71fb9"
      },
      "source": [
        "print(VT.round(2))\n",
        "np.shape(VT)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
            " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
            " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
            " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
            " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
            " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
            " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
            " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
            " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDZvRKD_KZPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01a7eb82-b387-41d0-aa73-9a2c243e30dc"
      },
      "source": [
        "#U × S × VT를 하면 기존의 행렬 A가 나와야 함.\n",
        "# Full SVD 수행물\n",
        "np.allclose(A, np.dot(np.dot(U,S), VT).round(2)) #allclose()는 2개의 행렬이 동일하면 True 반환"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RPMtuwhLDCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "463ee7a0-898a-4157-a349-e82b98d944e1"
      },
      "source": [
        "# 절단된 SVD 수행\n",
        "\n",
        "S = S[:2,:2] # 상위 2개만 남기고 제거\n",
        "print(S.round(2))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.69 0.  ]\n",
            " [0.   2.05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a5rw6erLfho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "47158dce-b1ad-4201-b8d5-b2fc482e07c9"
      },
      "source": [
        "U = U[:,:2]\n",
        "print(U.round(2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.24  0.75]\n",
            " [-0.51  0.44]\n",
            " [-0.83 -0.49]\n",
            " [-0.   -0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU2O5UIeLmgx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "63a577a9-5086-4bf9-de63-6c8fe562a298"
      },
      "source": [
        "VT=VT[:2,:] # 전치행렬 2개의 행만 남기고 제거, V 관점에서는 2개의 열만 남기고 제거한 거랑 같음\n",
        "print(VT.round(2))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
            " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c9ly8cxMbXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "8ad46a1e-a7e1-4509-d294-0b8c50a2c2df"
      },
      "source": [
        "A_prime = np.dot(np.dot(U,S), VT)\n",
        "print(A)\n",
        "print(A_prime.round(2))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 0 1 1 0 0]\n",
            " [0 0 0 1 1 0 1 0 0]\n",
            " [0 1 1 0 2 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 1 1]]\n",
            "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
            " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
            " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
            " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxucBAvPN4Xu",
        "colab_type": "text"
      },
      "source": [
        "**사이킷런을 통한 토픽 모델링 실습**\n",
        "\n",
        "LSA를 사용해서 원하는 토픽의 수로 압축하고, 각 토픽당 가장 중요한 단어 5개 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHIDKylYMn2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gmvGuyZOh-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43bba1f2-3d66-4ba4-ad05-bce8af519f5b"
      },
      "source": [
        "len(documents)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SULr5q6oOpKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "1dfb9c50-bfa1-43de-9b2f-5d89d44b26f6"
      },
      "source": [
        "documents[1]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDliUg4COrrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6ca74eec-6ae7-47bb-b7b6-e23d213146e1"
      },
      "source": [
        "print(dataset.target_names) #어떤 20개의 카테고리를 가지고 있었는지 확인"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vgQNt8RO5UJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a1d0db5f-b01f-48c2-e2e5-8d5ab1794a78"
      },
      "source": [
        "# 텍스트 전처리 하기\n",
        "\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "news_df[1:5]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although I realize that principle is not one o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well, I will have to change the scoring on my ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            document\n",
              "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
              "2  Although I realize that principle is not one o...\n",
              "3  Notwithstanding all the legitimate fuss about ...\n",
              "4  Well, I will have to change the scoring on my ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8jaZ-8uPbH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "56ea5883-a006-46ae-b53c-8240440ba4dd"
      },
      "source": [
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") #특수문자 제거\n",
        "news_df[1:5]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>clean_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
              "      <td>Yeah  do you expect people to read the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although I realize that principle is not one o...</td>\n",
              "      <td>Although I realize that principle is not one o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
              "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Well, I will have to change the scoring on my ...</td>\n",
              "      <td>Well  I will have to change the scoring on my ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            document                                          clean_doc\n",
              "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...         Yeah  do you expect people to read the ...\n",
              "2  Although I realize that principle is not one o...  Although I realize that principle is not one o...\n",
              "3  Notwithstanding all the legitimate fuss about ...  Notwithstanding all the legitimate fuss about ...\n",
              "4  Well, I will have to change the scoring on my ...  Well  I will have to change the scoring on my ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPkiJpWBPcFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "0b7cfcac-50b4-4c3e-eae7-b2dc4149b675"
      },
      "source": [
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) #길이가 3이하 단어 제거\n",
        "news_df['clean_doc'][1:5]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    Yeah expect people read actually accept hard a...\n",
              "2    Although realize that principle your strongest...\n",
              "3    Notwithstanding legitimate fuss about this pro...\n",
              "4    Well will have change scoring playoff pool Unf...\n",
              "Name: clean_doc, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZHOQ50hQagC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "22504bba-c554-4c78-86a6-e345260218db"
      },
      "source": [
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower()) #소문자로 바꾸기\n",
        "news_df['clean_doc'][1:5]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    yeah expect people read actually accept hard a...\n",
              "2    although realize that principle your strongest...\n",
              "3    notwithstanding legitimate fuss about this pro...\n",
              "4    well will have change scoring playoff pool unf...\n",
              "Name: clean_doc, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrvYhVZAQ_pY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "87623b3b-ff25-4bab-b2db-45a341d4278b"
      },
      "source": [
        "#토큰화 수행 후 불용어 제거하기\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "  \n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc[1:5]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    [yeah, expect, people, read, actually, accept,...\n",
              "2    [although, realize, that, principle, your, str...\n",
              "3    [notwithstanding, legitimate, fuss, about, thi...\n",
              "4    [well, will, have, change, scoring, playoff, p...\n",
              "Name: clean_doc, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBJTuV_4RbBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj_6oEqGRu1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5a31a5af-c863-4d63-daa9-1ce8428c3265"
      },
      "source": [
        "print(tokenized_doc[1]) # 토큰화 수행 후 your about just 등 불용어 제거됨"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1x7jH4MRyjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "59f78404-7466-4a5c-cdcb-6c973211979a"
      },
      "source": [
        "# TF-IDF는 토큰화가 되어 있지 않은 텍스트 데이터를 입력으로 사용하기 때문에, 다시 역토큰화(detoken~~) 해야한다.\n",
        "detokenized_doc = []\n",
        "for i in range(len(news_df)):\n",
        "  t = ' '.join(tokenized_doc[i])\n",
        "  detokenized_doc.append(t)\n",
        "\n",
        "news_df['clean_doc'] = detokenized_doc\n",
        "news_df['clean_doc'][1]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb_E1NxpTnfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 사이킷 런의 TfidfVectorizer를 통해 단어 1000개에 대한 TF-IDF 행렬 만든다.\n",
        "# 1000개 단어로 제한해서 실습해보겠음\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features = 1000, max_df = 0.5, smooth_idf=True)\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4nakpJLUJqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b36b752-3e8e-4f99-94ba-1120d77d3bd0"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDJFClYCWOfI",
        "colab_type": "text"
      },
      "source": [
        "**토픽 모델링하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4gOwf4SUXF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec5e948d-48c1-45f3-b47c-997b2ded1a04"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "svd_model.fit(X)\n",
        "len(svd_model.components_) #svd_model.components_ 이건 LSA의 VT에 해당 전치행렬"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7CGGV5LWjiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80b9573b-41e6-4ca1-ccb3-512f9a7e30e2"
      },
      "source": [
        "np.shape(svd_model.components_) #사전에 정의한 토픽수 20개. 단어수 1000개"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rXzccKJY8UJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "18315da1-e173-4b65-a0b6-2fa0ce629dc5"
      },
      "source": [
        "svd_model.components_"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.01469447,  0.05019033,  0.02132607, ...,  0.07865965,\n",
              "         0.01432356,  0.01788786],\n",
              "       [-0.00535706,  0.01653897, -0.01644943, ..., -0.0635444 ,\n",
              "        -0.01063538, -0.01905089],\n",
              "       [ 0.00172364, -0.00364954, -0.01799981, ...,  0.05876243,\n",
              "         0.0262722 ,  0.02236444],\n",
              "       ...,\n",
              "       [-0.01119135,  0.00486264,  0.00307659, ...,  0.01869445,\n",
              "        -0.00064808,  0.00032117],\n",
              "       [ 0.00206029,  0.01546828,  0.01100956, ..., -0.09165235,\n",
              "        -0.00111648, -0.00530245],\n",
              "       [ 0.00197576, -0.03695809, -0.00546641, ...,  0.03533267,\n",
              "        -0.01475702, -0.0039975 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqEtwn1qW2KC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPmswIPzXon_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "06f3e2a6-5f44-411f-9144-cd4778feed96"
      },
      "source": [
        "get_topics(svd_model.components_, terms)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
            "Topic 2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
            "Topic 3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
            "Topic 4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
            "Topic 5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
            "Topic 6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
            "Topic 7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
            "Topic 8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
            "Topic 9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
            "Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
            "Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
            "Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
            "Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
            "Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
            "Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
            "Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
            "Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
            "Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
            "Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
            "Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWL0lLhyZgZ9",
        "colab_type": "text"
      },
      "source": [
        "LSA는 문서의 유사도 계산을 빠르고 쉽게 구현이 가능한 장점을 가진다.\n",
        "\n",
        "하지만 새로운 데이터가 추가되면 처음부터 계산을 다시 해야 하기 때문에 업데이트가 어렵다.\n",
        "\n",
        "그래서 Word2Vec 등 단어의 의미를 벡터화할 수 있는 방법론이 각광받고 있다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaTYOcpFZIJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}